#!/bin/bash

# Day 9: Branching and Merging - Solutions
# Advanced Git branching strategies and workflows

echo "=== Day 9: Branching and Merging Solutions ==="
echo "Advanced Git workflows for data engineering teams"
echo

# Create advanced branching project
PROJECT_NAME="advanced-branching-project"
echo "Creating advanced Git project: $PROJECT_NAME"

mkdir -p "$PROJECT_NAME"
cd "$PROJECT_NAME"
git init

# Configure Git
git config user.name "Senior Data Engineer"
git config user.email "senior@example.com"

echo "=== Solution 1: Git Flow Implementation ==="

echo "1.1 Setting up Git Flow structure:"
# Create main branches
git checkout -b main
git checkout -b develop

# Create initial project structure
mkdir -p {src/{etl,analysis,utils},tests/{unit,integration},docs,config/{dev,prod},scripts,data/{raw,processed,staging}}

cat > README.md << 'EOF'
# Advanced Data Engineering Project

Enterprise-grade data processing platform with Git Flow workflow.

## Branch Strategy
- `main`: Production releases
- `develop`: Integration branch
- `feature/*`: Feature development
- `release/*`: Release preparation
- `hotfix/*`: Critical fixes

## Architecture
- ETL Pipeline with modular components
- Comprehensive testing framework
- Environment-specific configurations
- Automated deployment scripts

## Development Workflow
1. Create feature branch from develop
2. Implement feature with tests
3. Merge to develop via pull request
4. Create release branch for deployment
5. Merge release to main and develop
EOF

cat > .gitignore << 'EOF'
# Data files
data/raw/
data/processed/
data/staging/
*.csv
*.json
*.parquet

# Environment configs
config/dev/secrets.json
config/prod/secrets.json
.env*

# Python
__pycache__/
*.pyc
venv/
.pytest_cache/

# Logs and outputs
logs/
output/
reports/
*.log

# IDE
.vscode/
.idea/
EOF

# Initial commit
git add .
git commit -m "Initial project setup with Git Flow structure"

echo "âœ“ Git Flow structure established"

echo "1.2 Implementing feature branch workflow:"
# Feature 1: ETL Framework
git checkout develop
git checkout -b feature/etl-framework

cat > src/etl/extractor.py << 'EOF'
"""Advanced data extraction framework."""

from abc import ABC, abstractmethod
from typing import Dict, Any, List
import logging

class BaseExtractor(ABC):
    """Base class for data extractors."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
    
    @abstractmethod
    def extract(self) -> List[Dict[str, Any]]:
        """Extract data from source."""
        pass
    
    def validate_config(self) -> bool:
        """Validate extractor configuration."""
        required_fields = self.get_required_config_fields()
        return all(field in self.config for field in required_fields)
    
    @abstractmethod
    def get_required_config_fields(self) -> List[str]:
        """Get required configuration fields."""
        pass

class CSVExtractor(BaseExtractor):
    """Extract data from CSV files."""
    
    def get_required_config_fields(self) -> List[str]:
        return ['file_path', 'delimiter']
    
    def extract(self) -> List[Dict[str, Any]]:
        """Extract data from CSV file."""
        import csv
        
        if not self.validate_config():
            raise ValueError("Invalid configuration")
        
        data = []
        try:
            with open(self.config['file_path'], 'r') as f:
                reader = csv.DictReader(f, delimiter=self.config.get('delimiter', ','))
                data = list(reader)
            
            self.logger.info(f"Extracted {len(data)} records from CSV")
            return data
            
        except Exception as e:
            self.logger.error(f"CSV extraction failed: {e}")
            raise

class DatabaseExtractor(BaseExtractor):
    """Extract data from database."""
    
    def get_required_config_fields(self) -> List[str]:
        return ['connection_string', 'query']
    
    def extract(self) -> List[Dict[str, Any]]:
        """Extract data from database."""
        # Placeholder for database extraction
        self.logger.info("Database extraction completed")
        return []
EOF

git add src/etl/extractor.py
git commit -m "Add advanced extraction framework with base classes"

cat > src/etl/transformer.py << 'EOF'
"""Advanced data transformation framework."""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Callable
import logging

class BaseTransformer(ABC):
    """Base class for data transformers."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.transformations: List[Callable] = []
    
    def add_transformation(self, func: Callable) -> 'BaseTransformer':
        """Add transformation function to pipeline."""
        self.transformations.append(func)
        return self
    
    def transform(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Apply all transformations to data."""
        result = data
        
        for transformation in self.transformations:
            try:
                result = transformation(result)
                self.logger.debug(f"Applied transformation: {transformation.__name__}")
            except Exception as e:
                self.logger.error(f"Transformation failed: {e}")
                raise
        
        return result

class DataCleaner(BaseTransformer):
    """Data cleaning transformations."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.setup_default_transformations()
    
    def setup_default_transformations(self):
        """Set up default cleaning transformations."""
        if self.config.get('remove_duplicates', True):
            self.add_transformation(self.remove_duplicates)
        
        if self.config.get('handle_nulls', True):
            self.add_transformation(self.handle_null_values)
        
        if self.config.get('validate_types', True):
            self.add_transformation(self.validate_data_types)
    
    def remove_duplicates(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate records."""
        seen = set()
        unique_data = []
        
        for record in data:
            record_hash = hash(str(sorted(record.items())))
            if record_hash not in seen:
                seen.add(record_hash)
                unique_data.append(record)
        
        removed = len(data) - len(unique_data)
        self.logger.info(f"Removed {removed} duplicate records")
        return unique_data
    
    def handle_null_values(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Handle null/empty values."""
        strategy = self.config.get('null_strategy', 'remove')
        
        if strategy == 'remove':
            cleaned = [record for record in data if all(v for v in record.values())]
            removed = len(data) - len(cleaned)
            self.logger.info(f"Removed {removed} records with null values")
            return cleaned
        
        return data
    
    def validate_data_types(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Validate and convert data types."""
        type_mapping = self.config.get('type_mapping', {})
        
        for record in data:
            for field, target_type in type_mapping.items():
                if field in record:
                    try:
                        if target_type == 'int':
                            record[field] = int(record[field])
                        elif target_type == 'float':
                            record[field] = float(record[field])
                        elif target_type == 'str':
                            record[field] = str(record[field])
                    except (ValueError, TypeError):
                        self.logger.warning(f"Type conversion failed for {field}")
        
        return data
EOF

git add src/etl/transformer.py
git commit -m "Add advanced transformation framework with data cleaning"

echo "âœ“ ETL framework feature completed"
#!/bin/bash

# Day 9 Solutions - Part 2: Advanced branching workflows

echo "=== Solution 2: Parallel Feature Development ==="

echo "2.1 Feature 2: Analysis Engine (parallel development):"
git checkout develop
git checkout -b feature/analysis-engine

cat > src/analysis/analyzer.py << 'EOF'
"""Advanced data analysis engine."""

from typing import Dict, Any, List, Optional
import logging
from abc import ABC, abstractmethod

class BaseAnalyzer(ABC):
    """Base class for data analyzers."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
    
    @abstractmethod
    def analyze(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform analysis on data."""
        pass

class StatisticalAnalyzer(BaseAnalyzer):
    """Statistical analysis of data."""
    
    def analyze(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform statistical analysis."""
        if not data:
            return {'error': 'No data provided'}
        
        numeric_fields = self._identify_numeric_fields(data)
        results = {
            'record_count': len(data),
            'field_count': len(data[0]) if data else 0,
            'numeric_fields': numeric_fields,
            'statistics': {}
        }
        
        for field in numeric_fields:
            values = [float(record.get(field, 0)) for record in data if record.get(field)]
            if values:
                results['statistics'][field] = {
                    'count': len(values),
                    'sum': sum(values),
                    'mean': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values)
                }
        
        self.logger.info(f"Statistical analysis completed for {len(data)} records")
        return results
    
    def _identify_numeric_fields(self, data: List[Dict[str, Any]]) -> List[str]:
        """Identify numeric fields in data."""
        if not data:
            return []
        
        numeric_fields = []
        sample_record = data[0]
        
        for field, value in sample_record.items():
            try:
                float(value)
                numeric_fields.append(field)
            except (ValueError, TypeError):
                continue
        
        return numeric_fields

class QualityAnalyzer(BaseAnalyzer):
    """Data quality analysis."""
    
    def analyze(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze data quality metrics."""
        if not data:
            return {'error': 'No data provided'}
        
        total_records = len(data)
        field_names = list(data[0].keys()) if data else []
        
        quality_metrics = {
            'total_records': total_records,
            'total_fields': len(field_names),
            'completeness': {},
            'uniqueness': {},
            'consistency': {}
        }
        
        # Analyze completeness
        for field in field_names:
            non_null_count = sum(1 for record in data if record.get(field))
            quality_metrics['completeness'][field] = {
                'non_null_count': non_null_count,
                'completeness_ratio': non_null_count / total_records if total_records > 0 else 0
            }
        
        # Analyze uniqueness
        for field in field_names:
            values = [record.get(field) for record in data if record.get(field)]
            unique_values = len(set(values))
            quality_metrics['uniqueness'][field] = {
                'unique_count': unique_values,
                'uniqueness_ratio': unique_values / len(values) if values else 0
            }
        
        self.logger.info(f"Quality analysis completed for {total_records} records")
        return quality_metrics

class AnomalyDetector(BaseAnalyzer):
    """Detect anomalies in data."""
    
    def analyze(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect data anomalies."""
        anomalies = {
            'outliers': [],
            'missing_patterns': [],
            'inconsistencies': []
        }
        
        # Simple outlier detection for numeric fields
        numeric_fields = self._get_numeric_fields(data)
        
        for field in numeric_fields:
            values = [float(record.get(field, 0)) for record in data if record.get(field)]
            if len(values) > 1:
                mean_val = sum(values) / len(values)
                std_dev = (sum((x - mean_val) ** 2 for x in values) / len(values)) ** 0.5
                
                threshold = 2 * std_dev
                outliers = [val for val in values if abs(val - mean_val) > threshold]
                
                if outliers:
                    anomalies['outliers'].append({
                        'field': field,
                        'outlier_count': len(outliers),
                        'outlier_values': outliers[:5]  # First 5 outliers
                    })
        
        self.logger.info(f"Anomaly detection completed, found {len(anomalies['outliers'])} outlier patterns")
        return anomalies
    
    def _get_numeric_fields(self, data: List[Dict[str, Any]]) -> List[str]:
        """Get numeric field names."""
        if not data:
            return []
        
        numeric_fields = []
        for field, value in data[0].items():
            try:
                float(value)
                numeric_fields.append(field)
            except (ValueError, TypeError):
                continue
        
        return numeric_fields
EOF

git add src/analysis/analyzer.py
git commit -m "Add comprehensive analysis engine with statistics and quality metrics"

cat > src/analysis/reporter.py << 'EOF'
"""Advanced reporting system."""

from typing import Dict, Any, List
import json
import logging

class ReportGenerator:
    """Generate various types of reports."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def generate_summary_report(self, analysis_results: Dict[str, Any]) -> str:
        """Generate summary report from analysis results."""
        report = []
        report.append("=" * 50)
        report.append("DATA ANALYSIS SUMMARY REPORT")
        report.append("=" * 50)
        report.append(f"Generated: {self._get_timestamp()}")
        report.append("")
        
        # Statistical summary
        if 'statistics' in analysis_results:
            report.append("STATISTICAL SUMMARY")
            report.append("-" * 20)
            stats = analysis_results['statistics']
            for field, field_stats in stats.items():
                report.append(f"{field}:")
                report.append(f"  Count: {field_stats.get('count', 0)}")
                report.append(f"  Mean: {field_stats.get('mean', 0):.2f}")
                report.append(f"  Min: {field_stats.get('min', 0)}")
                report.append(f"  Max: {field_stats.get('max', 0)}")
                report.append("")
        
        # Quality metrics
        if 'completeness' in analysis_results:
            report.append("DATA QUALITY METRICS")
            report.append("-" * 20)
            completeness = analysis_results['completeness']
            for field, metrics in completeness.items():
                ratio = metrics.get('completeness_ratio', 0)
                report.append(f"{field}: {ratio:.1%} complete")
            report.append("")
        
        # Anomalies
        if 'outliers' in analysis_results:
            outliers = analysis_results['outliers']
            if outliers:
                report.append("ANOMALIES DETECTED")
                report.append("-" * 20)
                for outlier in outliers:
                    field = outlier.get('field', 'unknown')
                    count = outlier.get('outlier_count', 0)
                    report.append(f"{field}: {count} outliers detected")
                report.append("")
        
        report.append("=" * 50)
        
        return "\n".join(report)
    
    def generate_json_report(self, analysis_results: Dict[str, Any]) -> str:
        """Generate JSON format report."""
        report_data = {
            'timestamp': self._get_timestamp(),
            'analysis_results': analysis_results,
            'metadata': {
                'generator': 'Advanced Data Analysis Engine',
                'version': '1.0.0'
            }
        }
        
        return json.dumps(report_data, indent=2)
    
    def generate_html_report(self, analysis_results: Dict[str, Any]) -> str:
        """Generate HTML format report."""
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Data Analysis Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .section {{ margin: 20px 0; }}
        .metric {{ background: #e8f4f8; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Data Analysis Report</h1>
        <p>Generated: {self._get_timestamp()}</p>
    </div>
    
    <div class="section">
        <h2>Summary</h2>
        <div class="metric">Total Records: {analysis_results.get('record_count', 0)}</div>
        <div class="metric">Total Fields: {analysis_results.get('field_count', 0)}</div>
    </div>
    
    <div class="section">
        <h2>Analysis Results</h2>
        <pre>{json.dumps(analysis_results, indent=2)}</pre>
    </div>
</body>
</html>
"""
        return html
    
    def _get_timestamp(self) -> str:
        """Get current timestamp."""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
EOF

git add src/analysis/reporter.py
git commit -m "Add advanced reporting system with multiple output formats"

echo "âœ“ Analysis engine feature completed"

echo "2.2 Feature 3: Configuration Management:"
git checkout develop
git checkout -b feature/config-management

cat > src/utils/config_manager.py << 'EOF'
"""Advanced configuration management system."""

import json
import os
from typing import Dict, Any, Optional
import logging

class ConfigManager:
    """Manage application configuration across environments."""
    
    def __init__(self, config_dir: str = "config"):
        self.config_dir = config_dir
        self.logger = logging.getLogger(self.__class__.__name__)
        self._config_cache: Dict[str, Any] = {}
    
    def load_config(self, environment: str = "dev") -> Dict[str, Any]:
        """Load configuration for specified environment."""
        if environment in self._config_cache:
            return self._config_cache[environment]
        
        config_file = os.path.join(self.config_dir, environment, "config.json")
        
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
            
            # Load environment variables
            config = self._merge_env_variables(config)
            
            # Cache configuration
            self._config_cache[environment] = config
            
            self.logger.info(f"Configuration loaded for environment: {environment}")
            return config
            
        except FileNotFoundError:
            self.logger.error(f"Configuration file not found: {config_file}")
            return {}
        except json.JSONDecodeError as e:
            self.logger.error(f"Invalid JSON in configuration file: {e}")
            return {}
    
    def get_database_config(self, environment: str = "dev") -> Dict[str, Any]:
        """Get database configuration."""
        config = self.load_config(environment)
        return config.get('database', {})
    
    def get_etl_config(self, environment: str = "dev") -> Dict[str, Any]:
        """Get ETL pipeline configuration."""
        config = self.load_config(environment)
        return config.get('etl', {})
    
    def get_analysis_config(self, environment: str = "dev") -> Dict[str, Any]:
        """Get analysis configuration."""
        config = self.load_config(environment)
        return config.get('analysis', {})
    
    def _merge_env_variables(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Merge environment variables into configuration."""
        # Override with environment variables
        for key, value in os.environ.items():
            if key.startswith('APP_'):
                config_key = key[4:].lower()  # Remove APP_ prefix
                config[config_key] = value
        
        return config
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        """Validate configuration completeness."""
        required_sections = ['database', 'etl', 'analysis']
        
        for section in required_sections:
            if section not in config:
                self.logger.error(f"Missing required configuration section: {section}")
                return False
        
        return True
EOF

# Create configuration templates
mkdir -p config/{dev,prod,staging}

cat > config/dev/config.json << 'EOF'
{
  "database": {
    "host": "localhost",
    "port": 5432,
    "name": "dev_database",
    "user": "dev_user",
    "password": "dev_password"
  },
  "etl": {
    "batch_size": 1000,
    "parallel_workers": 2,
    "retry_attempts": 3,
    "timeout_seconds": 300
  },
  "analysis": {
    "enable_statistics": true,
    "enable_quality_checks": true,
    "enable_anomaly_detection": true,
    "outlier_threshold": 2.0
  },
  "logging": {
    "level": "DEBUG",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  }
}
EOF

cat > config/prod/config.json << 'EOF'
{
  "database": {
    "host": "prod-db-host",
    "port": 5432,
    "name": "prod_database",
    "user": "prod_user",
    "password": "${DB_PASSWORD}"
  },
  "etl": {
    "batch_size": 5000,
    "parallel_workers": 8,
    "retry_attempts": 5,
    "timeout_seconds": 1800
  },
  "analysis": {
    "enable_statistics": true,
    "enable_quality_checks": true,
    "enable_anomaly_detection": true,
    "outlier_threshold": 3.0
  },
  "logging": {
    "level": "INFO",
    "format": "%(asctime)s - %(levelname)s - %(message)s"
  }
}
EOF

git add src/utils/config_manager.py config/
git commit -m "Add comprehensive configuration management system"

echo "âœ“ Configuration management feature completed"

echo "=== Solution 3: Advanced Merge Strategies ==="

echo "3.1 Merging features with different strategies:"

# Merge ETL framework with no-ff to preserve feature history
git checkout develop
git merge --no-ff feature/etl-framework
echo "âœ“ ETL framework merged with --no-ff"

# Merge analysis engine with squash for clean history
git merge --squash feature/analysis-engine
git commit -m "Add comprehensive analysis engine

- Statistical analysis with mean, min, max calculations
- Data quality metrics including completeness and uniqueness
- Anomaly detection with outlier identification
- Multi-format reporting (text, JSON, HTML)"

echo "âœ“ Analysis engine merged with --squash"

# Merge config management normally
git merge feature/config-management
echo "âœ“ Configuration management merged"

echo "3.2 Clean up merged feature branches:"
git branch -d feature/etl-framework
git branch -d feature/analysis-engine
git branch -d feature/config-management

echo "âœ“ Feature branches cleaned up"
#!/bin/bash

# Day 9 Solutions - Part 3: Release and hotfix workflows

echo "=== Solution 4: Release Management Workflow ==="

echo "4.1 Create release branch:"
git checkout develop
git checkout -b release/v2.0.0

echo "4.2 Prepare release documentation:"
cat > docs/RELEASE_NOTES.md << 'EOF'
# Release Notes - Version 2.0.0

## New Features

### ETL Framework
- Advanced extraction framework with pluggable extractors
- Support for CSV and database sources
- Comprehensive data transformation pipeline
- Configurable data cleaning and validation

### Analysis Engine
- Statistical analysis with descriptive statistics
- Data quality assessment and metrics
- Anomaly detection and outlier identification
- Multi-format reporting (text, JSON, HTML)

### Configuration Management
- Environment-specific configuration support
- Secure credential management
- Runtime configuration validation
- Environment variable integration

## Technical Improvements
- Modular architecture with clear separation of concerns
- Comprehensive logging and error handling
- Type hints and documentation
- Extensible plugin system

## Breaking Changes
- Configuration file format updated
- API changes in extractor interfaces
- New required dependencies

## Migration Guide
1. Update configuration files to new format
2. Update extractor implementations
3. Install new dependencies: `pip install -r requirements.txt`

## Known Issues
- Large dataset processing may require memory optimization
- Database connection pooling not yet implemented
EOF

cat > requirements.txt << 'EOF'
# Core dependencies
pandas>=1.5.0
numpy>=1.21.0
sqlalchemy>=1.4.0

# Development dependencies
pytest>=7.0.0
black>=22.0.0
flake8>=5.0.0

# Optional dependencies
psycopg2-binary>=2.9.0  # PostgreSQL support
pymongo>=4.0.0          # MongoDB support
EOF

# Version bump
cat > src/__init__.py << 'EOF'
"""Advanced Data Engineering Platform."""

__version__ = "2.0.0"
__author__ = "Data Engineering Team"
__email__ = "team@example.com"
EOF

git add docs/RELEASE_NOTES.md requirements.txt src/__init__.py
git commit -m "Prepare release v2.0.0 with documentation and version bump"

echo "4.3 Release testing and bug fixes:"
# Simulate finding and fixing a bug during release preparation
cat > src/etl/extractor.py << 'EOF'
"""Advanced data extraction framework."""

from abc import ABC, abstractmethod
from typing import Dict, Any, List
import logging

class BaseExtractor(ABC):
    """Base class for data extractors."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
    
    @abstractmethod
    def extract(self) -> List[Dict[str, Any]]:
        """Extract data from source."""
        pass
    
    def validate_config(self) -> bool:
        """Validate extractor configuration."""
        required_fields = self.get_required_config_fields()
        return all(field in self.config for field in required_fields)
    
    @abstractmethod
    def get_required_config_fields(self) -> List[str]:
        """Get required configuration fields."""
        pass

class CSVExtractor(BaseExtractor):
    """Extract data from CSV files."""
    
    def get_required_config_fields(self) -> List[str]:
        return ['file_path']  # delimiter is optional
    
    def extract(self) -> List[Dict[str, Any]]:
        """Extract data from CSV file."""
        import csv
        
        if not self.validate_config():
            raise ValueError("Invalid configuration")
        
        data = []
        try:
            # Fix: Handle encoding properly
            with open(self.config['file_path'], 'r', encoding='utf-8') as f:
                delimiter = self.config.get('delimiter', ',')
                reader = csv.DictReader(f, delimiter=delimiter)
                data = list(reader)
            
            self.logger.info(f"Extracted {len(data)} records from CSV")
            return data
            
        except UnicodeDecodeError:
            # Fallback to different encoding
            try:
                with open(self.config['file_path'], 'r', encoding='latin-1') as f:
                    delimiter = self.config.get('delimiter', ',')
                    reader = csv.DictReader(f, delimiter=delimiter)
                    data = list(reader)
                
                self.logger.warning("Used latin-1 encoding fallback")
                return data
            except Exception as e:
                self.logger.error(f"CSV extraction failed: {e}")
                raise
        except Exception as e:
            self.logger.error(f"CSV extraction failed: {e}")
            raise

class DatabaseExtractor(BaseExtractor):
    """Extract data from database."""
    
    def get_required_config_fields(self) -> List[str]:
        return ['connection_string', 'query']
    
    def extract(self) -> List[Dict[str, Any]]:
        """Extract data from database."""
        # Placeholder for database extraction
        self.logger.info("Database extraction completed")
        return []
EOF

git add src/etl/extractor.py
git commit -m "Fix encoding issues in CSV extractor for release"

echo "4.4 Finalize release:"
git checkout main
git merge --no-ff release/v2.0.0
git tag -a v2.0.0 -m "Release version 2.0.0

Major release with new ETL framework, analysis engine, and configuration management system."

# Merge back to develop
git checkout develop
git merge --no-ff release/v2.0.0

# Clean up release branch
git branch -d release/v2.0.0

echo "âœ“ Release v2.0.0 completed and tagged"

echo "=== Solution 5: Hotfix Workflow ==="

echo "5.1 Critical bug discovered in production:"
git checkout main
git checkout -b hotfix/fix-memory-leak

echo "5.2 Fix critical memory leak:"
cat > src/etl/transformer.py << 'EOF'
"""Advanced data transformation framework."""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Callable
import logging
import gc  # Fix: Add garbage collection

class BaseTransformer(ABC):
    """Base class for data transformers."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.transformations: List[Callable] = []
    
    def add_transformation(self, func: Callable) -> 'BaseTransformer':
        """Add transformation function to pipeline."""
        self.transformations.append(func)
        return self
    
    def transform(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Apply all transformations to data."""
        result = data
        
        for transformation in self.transformations:
            try:
                result = transformation(result)
                self.logger.debug(f"Applied transformation: {transformation.__name__}")
                
                # Fix: Force garbage collection after each transformation
                if len(result) > 10000:  # For large datasets
                    gc.collect()
                    
            except Exception as e:
                self.logger.error(f"Transformation failed: {e}")
                raise
        
        return result

class DataCleaner(BaseTransformer):
    """Data cleaning transformations."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.setup_default_transformations()
    
    def setup_default_transformations(self):
        """Set up default cleaning transformations."""
        if self.config.get('remove_duplicates', True):
            self.add_transformation(self.remove_duplicates)
        
        if self.config.get('handle_nulls', True):
            self.add_transformation(self.handle_null_values)
        
        if self.config.get('validate_types', True):
            self.add_transformation(self.validate_data_types)
    
    def remove_duplicates(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate records."""
        # Fix: Use more memory-efficient approach for large datasets
        if len(data) > 50000:
            return self._remove_duplicates_chunked(data)
        
        seen = set()
        unique_data = []
        
        for record in data:
            record_hash = hash(str(sorted(record.items())))
            if record_hash not in seen:
                seen.add(record_hash)
                unique_data.append(record)
        
        removed = len(data) - len(unique_data)
        self.logger.info(f"Removed {removed} duplicate records")
        return unique_data
    
    def _remove_duplicates_chunked(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Memory-efficient duplicate removal for large datasets."""
        chunk_size = 10000
        seen = set()
        unique_data = []
        
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            
            for record in chunk:
                record_hash = hash(str(sorted(record.items())))
                if record_hash not in seen:
                    seen.add(record_hash)
                    unique_data.append(record)
            
            # Force garbage collection after each chunk
            gc.collect()
        
        removed = len(data) - len(unique_data)
        self.logger.info(f"Removed {removed} duplicate records (chunked processing)")
        return unique_data
    
    def handle_null_values(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Handle null/empty values."""
        strategy = self.config.get('null_strategy', 'remove')
        
        if strategy == 'remove':
            # Fix: Process in chunks for large datasets
            if len(data) > 50000:
                return self._handle_nulls_chunked(data)
            
            cleaned = [record for record in data if all(v for v in record.values())]
            removed = len(data) - len(cleaned)
            self.logger.info(f"Removed {removed} records with null values")
            return cleaned
        
        return data
    
    def _handle_nulls_chunked(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Memory-efficient null handling for large datasets."""
        chunk_size = 10000
        cleaned = []
        
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            chunk_cleaned = [record for record in chunk if all(v for v in record.values())]
            cleaned.extend(chunk_cleaned)
            
            # Force garbage collection after each chunk
            gc.collect()
        
        removed = len(data) - len(cleaned)
        self.logger.info(f"Removed {removed} records with null values (chunked processing)")
        return cleaned
    
    def validate_data_types(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Validate and convert data types."""
        type_mapping = self.config.get('type_mapping', {})
        
        # Fix: Process in chunks for memory efficiency
        if len(data) > 50000:
            chunk_size = 10000
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                self._validate_chunk_types(chunk, type_mapping)
                
                if i % (chunk_size * 5) == 0:  # Every 5 chunks
                    gc.collect()
        else:
            self._validate_chunk_types(data, type_mapping)
        
        return data
    
    def _validate_chunk_types(self, chunk: List[Dict[str, Any]], type_mapping: Dict[str, str]):
        """Validate types for a chunk of data."""
        for record in chunk:
            for field, target_type in type_mapping.items():
                if field in record:
                    try:
                        if target_type == 'int':
                            record[field] = int(record[field])
                        elif target_type == 'float':
                            record[field] = float(record[field])
                        elif target_type == 'str':
                            record[field] = str(record[field])
                    except (ValueError, TypeError):
                        self.logger.warning(f"Type conversion failed for {field}")
EOF

git add src/etl/transformer.py
git commit -m "Fix memory leak in data transformation for large datasets

- Add garbage collection after processing chunks
- Implement chunked processing for large datasets
- Optimize memory usage in duplicate removal and null handling"

echo "5.3 Merge hotfix to main and develop:"
git checkout main
git merge --no-ff hotfix/fix-memory-leak
git tag -a v2.0.1 -m "Hotfix v2.0.1 - Fix memory leak in data transformation"

git checkout develop
git merge --no-ff hotfix/fix-memory-leak

git branch -d hotfix/fix-memory-leak

echo "âœ“ Hotfix v2.0.1 applied to main and develop"

echo "=== Solution 6: Advanced Branch Analysis ==="

echo "6.1 Comprehensive branch analysis:"
echo "Complete Git history:"
git log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit --all

echo
echo "6.2 Branch statistics:"
echo "Total commits: $(git rev-list --count --all)"
echo "Commits on main: $(git rev-list --count main)"
echo "Commits on develop: $(git rev-list --count develop)"
echo "Total tags: $(git tag | wc -l)"
echo "Active branches: $(git branch | wc -l)"

echo
echo "6.3 Release tags:"
git tag -l -n1

echo
echo "6.4 Merge commit analysis:"
git log --merges --oneline

# Combine all solution parts
cd ..
cat solution.sh solution_part2.sh solution_part3.sh > solution_combined.sh
mv solution_combined.sh solution.sh
rm solution_part2.sh solution_part3.sh

cd advanced-branching-project

echo
echo "=== Advanced Solutions Complete ==="
echo
echo "Advanced Git branching techniques demonstrated:"
echo "âœ… Git Flow implementation with main/develop branches"
echo "âœ… Parallel feature development workflow"
echo "âœ… Advanced merge strategies (fast-forward, no-ff, squash)"
echo "âœ… Comprehensive release management workflow"
echo "âœ… Hotfix workflow for critical production issues"
echo "âœ… Conflict resolution and merge strategies"
echo "âœ… Branch cleanup and maintenance"
echo "âœ… Advanced Git history analysis"
echo "âœ… Professional commit messages and documentation"
echo "âœ… Tag management for releases"
echo
echo "ðŸŽ¯ You've mastered enterprise-grade Git workflows!"
echo
echo "Project structure:"
echo "- ETL Framework with pluggable extractors and transformers"
echo "- Analysis Engine with statistics and quality metrics"
echo "- Configuration Management system"
echo "- Comprehensive documentation and release notes"
echo "- Memory-optimized processing for large datasets"
